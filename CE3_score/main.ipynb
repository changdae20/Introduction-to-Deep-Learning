{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow 는 2.3.1 버전으로 업그레이드 후 작성하여 주시기 바랍니다. \\\n",
    "tensorflow1을 이용하고 싶으신 분은 tensorflow.compat.v1을 import에서 사용하시면 됩니다.\\\n",
    "같이 첨부된 cheatsheet.ipynb에 이번과제 수행시 필요한 기능들에 대해서만 사용예시들을 적어놓았으므로 참고하시면 도움이 될 듯 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorflowCNN 클래스의 함수들이 주어진 설명에 맞게 동작하도록 코드를 작성하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorflowCNN(tf.Module):\n",
    "    def __init__(self, filter_list, learning_rate):\n",
    "        '''\n",
    "        filter_list를 저장해 놓고 __call__ 호출시 사용합니다.\n",
    "        filter_list의 마지막 텐서의 out channel수가 class의 개수가 됩니다. \n",
    "        learning_rate는 fit에서 training시에 사용합니다.\n",
    "        \n",
    "        Inputs: \n",
    "        - filter_list: [filter_height, filter_weight, in_channels, out_channels] 의 shape을 가진 텐서들의 리스트\n",
    "        - learning_rate: float 값으로 optimizer의 learning rate로 설정해 줍니다.\n",
    "        '''\n",
    "        self.filter_list = filter_list\n",
    "        self.lr = learning_rate\n",
    "        return\n",
    "    \n",
    "    def __call__(self, images):\n",
    "        '''\n",
    "        init에서 저장해 놓은 tensor들을 filter로 하여 convolution operation을 수행합니다.\n",
    "        마지막에 1, 2 axis방향으로 reduce_mean을 수행하여 [batch_size, optdim]의 shape을 가지도록 하고 1 axis로 softmax를 수행합니다.\n",
    "        convolution이외의 opearation은 사용하지 않습니다.\n",
    "        \n",
    "        Inputs: \n",
    "        - images: [batch_size, height, width, channel]의 shape을 가진 tensor입니다.\n",
    "        \n",
    "        Returns: \n",
    "        - result: convolution operation을 수행한 결과로 [batch_size, optdim]의 shape을 가진 tensor입니다.\n",
    "        '''\n",
    "        #### Method 1 : Just using operations ####\n",
    "        temp = tf.constant(images, dtype=tf.float32)\n",
    "        for i in range(np.shape(self.filter_list)[0]):\n",
    "            temp = tf.nn.conv2d(temp, self.filter_list[i], strides=[1,1,1,1], padding = \"VALID\")\n",
    "            temp = tf.nn.relu(temp)\n",
    "        temp = tf.math.reduce_mean(temp,axis=1)\n",
    "        temp = tf.math.reduce_mean(temp,axis=1)\n",
    "        result = tf.nn.softmax(temp,axis=1)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def fit(self, images, labels, epochs):\n",
    "        '''\n",
    "        images, labels을 이용하여 epochs 번 업데이트를 수행합니다.\n",
    "        loss는 cross entropy를 이용하여 optimizer는 SGD를 이용합니다.\n",
    "        learning_rate는 init에서 저장한 값을 이용합니다.\n",
    "        \n",
    "        Inputs: \n",
    "        - images: [batch_size, height, width, channel]의 shape을 가진 tensor입니다.\n",
    "        - labels: [batch_size]의 shape을 가진 integer-valed tensor입니다. 0이상 클래스 수 미만의 값을 가집니다.\n",
    "        - epochs: integer값으로 update step수를 나타냅니다.\n",
    "        \n",
    "        Returns: \n",
    "        - losses: float의 리스트로 \n",
    "        '''\n",
    "        \n",
    "        losses = np.zeros([epochs])\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = self.lr)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(labels, self(images)))\n",
    "            gradients = tape.gradient(loss, self.filter_list)\n",
    "            optimizer.apply_gradients(zip(gradients,self.filter_list))\n",
    "            losses[i]=loss.numpy()\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def save(self):\n",
    "        '''\n",
    "        saved_model이 아닌 checkpoint를 이용하여 저장합니다.\n",
    "        '''\n",
    "        self.ckpt_map1 = {'var' : self.filter_list, 'model' : self}\n",
    "        self.ckpt1 = tf.train.Checkpoint(**self.ckpt_map1)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(checkpoint=self.ckpt1,directory='ckpt',max_to_keep=1)\n",
    "        self.ckpt_manager.save()\n",
    "        return\n",
    "    \n",
    "    def restore(self):\n",
    "        '''\n",
    "        마지막 save함수가 호출 되었을 때의 값으로 variable의 값들을 set해 줍니다.\n",
    "        '''\n",
    "        self.ckpt1.restore(self.ckpt_manager.latest_checkpoint)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cifar100 dataset에 대해서 tensorflow를 이용하여 모델을 작성하고 training하여서 tf.saved_model.save(model, 'model')을 통해 저장하십시오.\n",
    "- training이 완료된 model은 프로젝트 폴더 하위에 위치한 'model' 디렉토리에 저장되어 있어야 합니다.\n",
    "- loaded_model = tf.saved_model.load('model')의 방법을 통해서 불러 올 수 있어야 하고 불러온 모델의 accuracy가 0.85 이상이면 정답으로 처리하겠습니다.\n",
    "- **model를 training을 완료하고 저장후에는 \"반드시 주석처리\" 하여서 notebook import시에 실행이 되지 않도록 해 주세요.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Start\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 8.8046\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 4.3612\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 4.0551\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 4.0780\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.7134\n",
      "epoch : 0\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.6159\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.9792\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4835\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.5991\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.4603\n",
      "epoch : 1\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.4702\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.8146\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.2239\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4947\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4541\n",
      "epoch : 2\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1266\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.5213\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 3.0321\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.3661\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.2881\n",
      "epoch : 3\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.0320\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.4652\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3.0887\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.3429\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1177\n",
      "epoch : 4\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.8815\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.4093\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.8962\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 3.1858\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 3.0198\n",
      "epoch : 5\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7545\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.2238\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.6498\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0224\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.8431\n",
      "epoch : 6\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.6896\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.3311\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.8422\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.1203\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.7257\n",
      "epoch : 7\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.7604\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.0629\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5590\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9128\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.8439\n",
      "epoch : 8\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5308\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0126\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.6234\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9559\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.8524\n",
      "epoch : 9\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.3299\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0069\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.6617\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.1556\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.7222\n",
      "epoch : 10\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.5378\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9131\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.6648\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9272\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.6288\n",
      "epoch : 11\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5450\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.9079\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.7240\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.8472\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.5350\n",
      "epoch : 12\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.4595\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.9392\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.5442\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.7521\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5206\n",
      "epoch : 13\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.1960\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 3.0277\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.2729\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.6072\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.5857\n",
      "epoch : 14\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.2522\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.9206\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.7521\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.7352\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.3966\n",
      "epoch : 15\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.1984\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.7617\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.4777\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.6705\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.3465\n",
      "epoch : 16\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.2343\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.6966\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.7382\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.4142\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.2765\n",
      "epoch : 17\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.6061\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.8657\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5163\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5622\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.3851\n",
      "epoch : 18\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.3267\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.6851\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5299\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.4445\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.1433\n",
      "epoch : 19\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.2620\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5566\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.4134\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5247\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.1748\n",
      "epoch : 20\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.3619\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.4555\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.2044\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.6957\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.1671\n",
      "epoch : 21\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.1513\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.4445\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.3203\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5991\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.0978\n",
      "epoch : 22\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.9657\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.6277\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.2203\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5113\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.9859\n",
      "epoch : 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 2.1623\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.3620\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.0885\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5671\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.9997\n",
      "epoch : 24\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.4428\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.3286\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.1706\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.6534\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.1358\n",
      "epoch : 25\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.1765\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.2236\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.0413\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.5051\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.8489\n",
      "epoch : 26\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.1482\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.9588\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.2787\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.3945\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.9108\n",
      "epoch : 27\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.0212\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.2156\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.0970\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 2.3678\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.9058\n",
      "epoch : 28\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.0026\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.3281\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.3172\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.4594\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.9472\n",
      "epoch : 29\n",
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    }
   ],
   "source": [
    "# train= tfds.load(name=\"cifar100\", split=\"train\", shuffle_files=True)#, as_supervised=True)\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Conv2D(32,kernel_size=[3, 3],activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "#     tf.keras.layers.Conv2D(64,kernel_size=[3, 3],activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "#     tf.keras.layers.Conv2D(128,kernel_size=[3, 3],activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(256,activation='relu'),\n",
    "#     tf.keras.layers.Dense(128,activation='relu'),\n",
    "#     tf.keras.layers.Dense(100,activation='softmax')\n",
    "# ])\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "\n",
    "\n",
    "# for epoch in range(30):\n",
    "#     for idx, batch in enumerate(train.batch(100)):\n",
    "#         image, label = batch[\"image\"], batch[\"label\"]\n",
    "#         image = tf.cast(image, tf.float32)\n",
    "#         model.fit(image,label, epochs=1, verbose=0)\n",
    "#         if(idx%100==0): model.fit(image,label, epochs=1, verbose=1)\n",
    "#     print(\"epoch : %d\"%(epoch))\n",
    "\n",
    "# tf.saved_model.save(model,\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로젝트 폴더를 HW3_20XX_XXXXX.zip으로 HW3_본인학번.zip 압축하여서 etl을 통해 제출하시면 됩니다.\\\n",
    "**notebook을 import하여 정상동작여부를 확인하므로 import문을 제외하고는 notebook import시에 실행되는 코드가 없도록 확인해 주세요**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
