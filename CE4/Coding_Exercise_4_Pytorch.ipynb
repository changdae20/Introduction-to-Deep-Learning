{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 Introduction to Deep Learning Coding Ex 4: Spam Message Generator!\n",
    "\n",
    "Contact T/A: Yeon-goon Kim, SNU ECE, CML. (ygoonkim@cml.snu.ac.kr)  \n",
    "\n",
    "Dataset from http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/\n",
    "\n",
    "On this homework, you will train spam message generator, which is basic RNN/LSTM/GRU char2char generation model. Of course, your results may not so good, but you can expect some sentence-like results by doing this homework sucessfully.\n",
    "\n",
    "## Now, We'll handle texts, not images. Is there any differences?\n",
    "\n",
    "Of course, there are many differences between processing images and texts. One is that text cannot be expressed directly in matrices or tensors. We know an image can be expressed in Tensor(n_channel, width, height). But how about an text sentence? Can word 'Homework' can be expressed in tensor directly? By what laws? With what shapes? Even if it can, which one is closer to the word, 'Burden', or 'Work'? This is called 'Word Embedding Problem' and be considered as one of the most important problem in Natural Language Process(NLP) resarch. Fortunatly, there are some generalized solution in this problem (though not prefect, anyway) and both Tensorflow and Pytorch give basic APIs that solve this problem automatically. You may use those APIs in this homework. \n",
    "\n",
    "The other one is that text is sequential data. Generally, when processing images, without batch, input is just one image. However in text, input is mostly some or one paragraphs/sentences, which is sequential data of embedded character or words. So, If we want to generate word 'Homework' with start token 'H', 'o' before 'H' and 'o' before 'Homew' should operate different when it gives to input. This is why we use RNN-based model in deep learning when processing text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement-Pytorch\n",
    "In this homework I recommend that you should use the latest stable version of Pytorch, which is on now(2020-11-05) Pytorch 1.7.x.. I'm using python3.7 on grading environment but there are no major changes on python3.6/8 so will not be a serious problem. \n",
    "There are other required packages to run the code which are 'unidecode' and 'pkbar'. You can easily install these packages with 'pip install unidecode' and 'pip install pkbar'.\n",
    "\n",
    "You can add other packages if you want, but if they are not basically given pkgs in Python/Pytorch you should contact T/A to check whether you can use them or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Import Packages ##########\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import random\n",
    "import pkbar\n",
    "import unidecode\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2020\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "############################# Changeable Parameters #############################\n",
    "USE_GPU = True\n",
    "SEQ_LENGTH = 200\n",
    "N_ITER = 50000\n",
    "TXT_GEN_PERIOD = 500\n",
    "LEARNING_RATE = 0.005\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "#################################################################################\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepration (Contact T/A If you wan to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./spam.txt', 'r') as f:\n",
    "    textfile = f.read()\n",
    "\n",
    "TEXT_LENGTH = len(textfile)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "textfile = unidecode.unidecode(textfile)\n",
    "textfile = re.sub(' +',' ', textfile)\n",
    "\n",
    "def pick_input(textfile):\n",
    "    start_index = random.randint(0, TEXT_LENGTH - SEQ_LENGTH)\n",
    "    end_index = start_index + SEQ_LENGTH + 1\n",
    "    return textfile[start_index:end_index]\n",
    "\n",
    "def char2tensor(text):\n",
    "    lst = [string.printable.index(c) for c in text]\n",
    "    tensor = torch.tensor(lst, device=DEVICE).long()\n",
    "    return tensor\n",
    "\n",
    "def draw_random_sample(textfile):    \n",
    "    sampled_seq = char2tensor(pick_input(textfile))\n",
    "    inputs = sampled_seq[:-1]\n",
    "    outputs = sampled_seq[1:]\n",
    "    return inputs, outputs\n",
    "\n",
    "print(draw_random_sample(textfile))\n",
    "print(draw_random_sample(textfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Preparation Functions\n",
    "You can add any other functions that preparation data in below cell.  \n",
    "\n",
    "However, you should annotate precisely for each functions you define. One annotation line should not cover more than 5 lines that you write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: RNN/LSTM/GRU Module\n",
    "\n",
    "The main task is to create RNN/LSTM/GRU network. You can use Pytorch api such as nn.XXX or barebone torch with F. I recommend using nn.XXX and module form that described on under, but you can use any pytorch api that basically given. \n",
    "\n",
    "Basically __init__ and forward are given, but you can add other functions that help your networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### WRITE DOWN YOUR CODE ################################\n",
    "\n",
    "class selfModule(nn.Module):\n",
    "    def __init__(self, args_you_want):\n",
    "        super(selfModule, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, args_you_want):        \n",
    "\n",
    "#################### WRITE DOWN YOUR CODE ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task: Train & Generate Code\n",
    "\n",
    "This cell would define functions of training network and generating text function. \n",
    "\n",
    "You can change these codes but if then you should annotate where do you make change precisely.\n",
    "One annotation line should not cover more than 5 lines that you make your changes.  \n",
    "Also, do not delete original code, just comment out them. (or make another cells of jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, generate_period, checkpoint_dir):\n",
    "    model.train()\n",
    "    kbar = pkbar.Kbar(target=generate_period, \n",
    "                              epoch=0,\n",
    "                              num_epochs=int(N_ITER/generate_period),\n",
    "                              width=50, \n",
    "                              always_stateful=True)\n",
    "    for i in range(N_ITER):        \n",
    "        hidden = model.init_zero_state()\n",
    "        optimizer.zero_grad()    \n",
    "        loss = 0.\n",
    "        x, y = draw_random_sample(textfile)\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        for c in range(SEQ_LENGTH):\n",
    "            outputs, hidden = model(x[c], hidden)\n",
    "            loss += F.cross_entropy(outputs, y[c].view(1))\n",
    "        loss /= SEQ_LENGTH\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        kbar.update((i % generate_period), values=[(\"t_loss\", loss)])\n",
    "        if (i % generate_period==0):\n",
    "            print(\"\\nIteration %d\"%i)\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_%d.pt\"%i)\n",
    "            torch.save(model.state_dict(), checkpoint_prefix)\n",
    "            print(test(model, 'joy', SEQ_LENGTH))\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "def test(model, prime_str, predict_len, temperature=0.8):\n",
    "    model.eval()\n",
    "    hidden = model.init_zero_state()\n",
    "    prime_input = char2tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_input[p].to(DEVICE), hidden.to(DEVICE))\n",
    "    inp = prime_input[-1]    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp.to(DEVICE), hidden.to(DEVICE))\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = string.printable[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char2tensor(predicted_char)\n",
    "    model.train()\n",
    "    return predicted        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Code & Credit Creterion\n",
    "Half Credit (4 points): Generate some ugly text, without any meaningful words.\n",
    "\n",
    "Q3 Credits (6 points): In SEQ_LENGTH 200, generate 6 or less differet words.\n",
    "\n",
    "Full Credit (8 points): in SEQ_LENGTH 200, generate 7 or more different words.\n",
    "\n",
    "\n",
    "\n",
    "You can change this cell based on your code modifications above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './pt-checkpoints'+ datetime.datetime.now().strftime(\"_%Y.%m.%d-%H:%M:%S\")\n",
    "os.mkdir(checkpoint_dir)\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = selfModule(args_you_want)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "train(model, TXT_GEN_PERIOD, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Code\n",
    "You can change this if you don't like or understand this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ckpt_dir = ''\n",
    "l_model = selfModule(args_you_want)\n",
    "l_model.load_state_dict(torch.load(load_ckpt_dir, map_location=device))\n",
    "l_model.eval()\n",
    "l_model.to(device=device)\n",
    "print(test(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-trained Networks: Transformers\n",
    "\n",
    "Actually, RNN-based sequential model is now regarded as little bit old-fashioned, since 'Transformer' model was announced in paper 'Attention is All You Need'(https://arxiv.org/abs/1706.03762). Now this model is widely used on many state-of-the-art sequential-data-use model, and even in non-sequential-data-use (ex)image) model too. However, model training cost is too heavy(maybe you need multiple million-won GPUs) to train on this homework. Fortunately, there is package called 'transformers' that contains multipe pre-trained transformer-based model that can be used directly. Below is example of text generation using GPT2, which is one of the most popular pre-trained NLP models.\n",
    "\n",
    "You can install this package with 'pip install transformers'. To download pre-traind model, you may have 2GB or more free disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "# Install ipywidgets and restart notebooks if you meet error message.\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(25)\n",
    "start_text = 'Fill this blank whatever you want.'\n",
    "generator(start_text, max_length=300, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2: Follow Tutorial of Fine-tuned Networks (2 points.)\n",
    "There are hundreds of fine-tuned NLP models in 'transformers' package. Try one of these models and follow its tutorial (except language translation model). Results must produce some meaningful, or funny one, and you must write down what model you choose and explain its function (ex) what is input/output, what does it mean etc) with one or two sentences. \n",
    "\n",
    "Hint: You can find list of pre-trained models in 'transformers' package on https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### WRITE DOWN YOUR CODE ################################\n",
    "from transformers import\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################### WRITE DOWN YOUR CODE ################################\n",
    "\n",
    "########### WRITE DOWN YOUR Explaination with annotation #################\n",
    "\n",
    "# Explaination: \n",
    "\n",
    "##########################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information: Project\n",
    "Of course, there are massive ammount of pretrained models on domain of image, NLP or else in web with open-source licenses. You can fine-tune those models if your GPUs are good enough, or at least transfer its information by using output feature of pre-trained networks. Or, maybe neither, it is up to you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CE4-torch",
   "language": "python",
   "name": "ce4-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
